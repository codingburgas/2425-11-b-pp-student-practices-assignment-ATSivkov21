# üì¶ –ò–º–ø–æ—Ä—Ç –Ω–∞ NumPy –∑–∞ —á–∏—Å–ª–æ–≤–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏ –º–∞—Å–∏–≤–∏
import numpy as np
# üíæ –ò–º–ø–æ—Ä—Ç –Ω–∞ joblib –∑–∞ –∑–∞–ø–∏—Å –∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏ –º–æ–¥–µ–ª–∏
import joblib
# üìÇ –ó–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞–Ω–µ –Ω–∞ —Ñ–∞–π–ª
import os
# üìå –ü—ä—Ç –¥–æ —Ñ–∞–π–ª–∞, –≤ –∫–æ–π—Ç–æ —Å–µ –ø–∞–∑–∏ –º–æ–¥–µ–ª—ä—Ç
MODEL_PATH = 'instance/model.pkl'

class SimpleLogisticRegression:
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ç–µ–≥–ª–∞—Ç–∞ –∏ bias-–∞ (—Ç–µ —â–µ —Å–µ –æ–±—É—á–∞—Ç –ø—Ä–∏ –Ω—É–∂–¥–∞)
        self.weights = None
        self.bias = None

    def sigmoid(self, z):
        # –°–∏–≥–º–æ–∏–¥–∞ ‚Äî –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞ –ª–æ–≥–∏—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å–∏—è
        return 1 / (1 + np.exp(-z))

    def predict_proba(self, X):
        # –ê–∫–æ –º–æ–¥–µ–ª—ä—Ç –Ω–µ –µ –æ–±—É—á–µ–Ω, –æ–±—É—á–∞–≤–∞–º–µ —Å dummy –¥–∞–Ω–Ω–∏
        if self.weights is None or self.bias is None:
            X_dummy = np.random.rand(10, X.shape[1])  # 10 –ø—Ä–∏–º–µ—Ä–∞ —Å—ä—Å —Å—ä—â–∏—Ç–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            y_dummy = (X_dummy[:, 0] + X_dummy[:, 1] > 1).astype(int)  # –ø—Ä–æ—Å—Ç–∞ –ª–æ–≥–∏–∫–∞ –∑–∞ target
            self.fit(X_dummy, y_dummy)

        # –í—Ä—ä—â–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ (–º–µ–∂–¥—É 0 –∏ 1)
        return self.sigmoid(np.dot(X, self.weights) + self.bias)

    def predict(self, X):
        # –í—Ä—ä—â–∞ 0 –∏–ª–∏ 1 —Å–ø–æ—Ä–µ–¥ —Ç–æ–≤–∞ –¥–∞–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ –µ >= 0.5
        return (self.predict_proba(X) >= 0.5).astype(int)

    def fit(self, X, y, lr=0.1, epochs=100):
        # –û–±—É—á–µ–Ω–∏–µ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–µ–Ω —Å–ø—É—Å–∫
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(epochs):
            linear_model = np.dot(X, self.weights) + self.bias
            y_predicted = self.sigmoid(linear_model)

            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–∏ –∑–∞ —Ç–µ–≥–ª–∞—Ç–∞ –∏ bias-–∞
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)

            # –û–±–Ω–æ–≤—è–≤–∞–Ω–µ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—Ç–µ
            self.weights -= lr * dw
            self.bias -= lr * db

    def save(self):
        # –ó–∞–ø–∏—Å–≤–∞–Ω–µ –Ω–∞ —Ç–µ–≥–ª–∞—Ç–∞ –∏ bias-–∞ –≤—ä–≤ —Ñ–∞–π–ª
        joblib.dump((self.weights, self.bias), MODEL_PATH)

    def load(self):
        # –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞, –∞–∫–æ —Ñ–∞–π–ª—ä—Ç —Å—ä—â–µ—Å—Ç–≤—É–≤–∞
        if os.path.exists(MODEL_PATH):
            self.weights, self.bias = joblib.load(MODEL_PATH)


model = SimpleLogisticRegression()

def predict_click_probability(survey):
    # üìè –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–Ω–µ: –±—Ä–æ–π —Å–∏–º–≤–æ–ª–∏ –≤ –∏–Ω—Ç–µ—Ä–µ—Å–∏—Ç–µ (–¥–æ 256)
    interests_len = len(survey.interests or "") / 256

    # üìä –ë—Ä–æ–π –∏–∑–±—Ä–∞–Ω–∏ —Ä–µ–∫–ª–∞–º–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–Ω (–ø—Ä–∏–µ–º–∞–º–µ, —á–µ —Å–∞ –¥–æ 3)
    ad_count = len((survey.selected_ads or "").split(',')) / 3

    # üíª –ö–æ–¥–∏—Ä–∞–Ω–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ—Ç–æ:
    # PC ‚Üí 0, Mobile ‚Üí 0.5, Tablet ‚Üí 1
    device_score = 0 if survey.device == 'PC' else (1 if survey.device == 'Mobile' else 2) / 2

    # üßÆ –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –≤—Ö–æ–¥–µ–Ω –≤–µ–∫—Ç–æ—Ä –∑–∞ –º–æ–¥–µ–ª–∞
    X = np.array([
        survey.age / 100,                  # –í—ä–∑—Ä–∞—Å—Ç—Ç–∞ –∫–∞—Ç–æ —Å—Ç–æ–π–Ω–æ—Å—Ç –æ—Ç 0 –¥–æ 1
        survey.daily_online_hours / 24,   # –û–Ω–ª–∞–π–Ω –≤—Ä–µ–º–µ –∫–∞—Ç–æ —Å—Ç–æ–π–Ω–æ—Å—Ç –æ—Ç 0 –¥–æ 1
        device_score,                     # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∫–∞—Ç–æ —á–∏—Å–ª–æ–≤–∞ —Å—Ç–æ–π–Ω–æ—Å—Ç
        interests_len,                    # –î—ä–ª–∂–∏–Ω–∞ –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å–∏—Ç–µ (0 –¥–æ 1)
        ad_count                          # –ë—Ä–æ–π —Ä–µ–∫–ª–∞–º–∏ (0 –¥–æ 1)
    ]).reshape(1, -1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–≤–∞–Ω–µ –≤—ä–≤ —Ñ–æ—Ä–º–∞—Ç [1, N]

    # –í—Ä—ä—â–∞–Ω–µ –Ω–∞ –∑–∞–∫—Ä—ä–≥–ª–µ–Ω–∞—Ç–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç (–ø—Ä–∏–º–µ—Ä–Ω–æ: 0.76)
    return round(float(model.predict_proba(X)[0]), 2)
