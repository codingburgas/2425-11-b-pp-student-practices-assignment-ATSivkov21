# üì¶ –ò–º–ø–æ—Ä—Ç –Ω–∞ NumPy –∑–∞ —á–∏—Å–ª–æ–≤–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏ –º–∞—Å–∏–≤–∏
import numpy as np
# üíæ –ò–º–ø–æ—Ä—Ç –Ω–∞ joblib –∑–∞ –∑–∞–ø–∏—Å –∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏ –º–æ–¥–µ–ª–∏
import joblib
# üìÇ –ó–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞–Ω–µ –Ω–∞ —Ñ–∞–π–ª
import os
# üìä –ó–∞ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏
from sklearn.metrics import accuracy_score, mean_squared_error, log_loss
# üìå –ü—ä—Ç –¥–æ —Ñ–∞–π–ª–∞, –≤ –∫–æ–π—Ç–æ —Å–µ –ø–∞–∑–∏ –º–æ–¥–µ–ª—ä—Ç
MODEL_PATH = 'instance/model.pkl'

class SimpleLogisticRegression:
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ç–µ–≥–ª–∞—Ç–∞ –∏ bias-–∞ (—Ç–µ —â–µ —Å–µ –æ–±—É—á–∞—Ç –ø—Ä–∏ –Ω—É–∂–¥–∞)
        self.weights = None
        self.bias = None
        self.training_history = {'loss': [], 'accuracy': []}

    def sigmoid(self, z):
        # –°–∏–≥–º–æ–∏–¥–∞ ‚Äî –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞ –ª–æ–≥–∏—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å–∏—è
        return 1 / (1 + np.exp(-z))

    def predict_proba(self, X):
        # –ê–∫–æ –º–æ–¥–µ–ª—ä—Ç –Ω–µ –µ –æ–±—É—á–µ–Ω, –æ–±—É—á–∞–≤–∞–º–µ —Å dummy –¥–∞–Ω–Ω–∏
        if self.weights is None or self.bias is None:
            X_dummy = np.random.rand(10, X.shape[1])  # 10 –ø—Ä–∏–º–µ—Ä–∞ —Å—ä—Å —Å—ä—â–∏—Ç–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            y_dummy = (X_dummy[:, 0] + X_dummy[:, 1] > 1).astype(int)  # –ø—Ä–æ—Å—Ç–∞ –ª–æ–≥–∏–∫–∞ –∑–∞ target
            self.fit(X_dummy, y_dummy)

        # –í—Ä—ä—â–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ (–º–µ–∂–¥—É 0 –∏ 1)
        return self.sigmoid(np.dot(X, self.weights) + self.bias)

    def predict(self, X):
        # –í—Ä—ä—â–∞ 0 –∏–ª–∏ 1 —Å–ø–æ—Ä–µ–¥ —Ç–æ–≤–∞ –¥–∞–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ –µ >= 0.5
        return (self.predict_proba(X) >= 0.5).astype(int)

    def fit(self, X, y, lr=0.1, epochs=100):
        # –û–±—É—á–µ–Ω–∏–µ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–µ–Ω —Å–ø—É—Å–∫
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # –ò–∑—á–∏—Å—Ç–≤–∞–Ω–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—è—Ç–∞ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ
        self.training_history = {'loss': [], 'accuracy': []}

        for epoch in range(epochs):
            linear_model = np.dot(X, self.weights) + self.bias
            y_predicted = self.sigmoid(linear_model)

            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–∏ –∑–∞ —Ç–µ–≥–ª–∞—Ç–∞ –∏ bias-–∞
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)

            # –û–±–Ω–æ–≤—è–≤–∞–Ω–µ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—Ç–µ
            self.weights -= lr * dw
            self.bias -= lr * db
            
            # –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
            if epoch % 10 == 0:  # –ó–∞–ø–∏—Å–≤–∞–º–µ –Ω–∞ –≤—Å–µ–∫–∏ 10 –µ–ø–æ—Ö–∏
                loss = log_loss(y, y_predicted)
                accuracy = accuracy_score(y, self.predict(X))
                self.training_history['loss'].append(loss)
                self.training_history['accuracy'].append(accuracy)

    def evaluate(self, X_test, y_test):
        """–û—Ü–µ–Ω—è–≤–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞ —Å —Ä–∞–∑–ª–∏—á–Ω–∏ –º–µ—Ç—Ä–∏–∫–∏"""
        y_pred = self.predict(X_test)
        y_pred_proba = self.predict_proba(X_test)
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'mse': mean_squared_error(y_test, y_pred_proba),
            'log_loss': log_loss(y_test, y_pred_proba),
            'training_loss': self.training_history['loss'][-1] if self.training_history['loss'] else None,
            'training_accuracy': self.training_history['accuracy'][-1] if self.training_history['accuracy'] else None
        }
        
        return metrics

    def save(self):
        # –ó–∞–ø–∏—Å–≤–∞–Ω–µ –Ω–∞ —Ç–µ–≥–ª–∞—Ç–∞ –∏ bias-–∞ –≤—ä–≤ —Ñ–∞–π–ª
        joblib.dump((self.weights, self.bias, self.training_history), MODEL_PATH)

    def load(self):
        # –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞, –∞–∫–æ —Ñ–∞–π–ª—ä—Ç —Å—ä—â–µ—Å—Ç–≤—É–≤–∞
        if os.path.exists(MODEL_PATH):
            loaded_data = joblib.load(MODEL_PATH)
            if len(loaded_data) == 3:
                self.weights, self.bias, self.training_history = loaded_data
            else:
                # Backward compatibility
                self.weights, self.bias = loaded_data
                self.training_history = {'loss': [], 'accuracy': []}


model = SimpleLogisticRegression()

def predict_click_probability(survey):
    # üìè –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–Ω–µ: –±—Ä–æ–π —Å–∏–º–≤–æ–ª–∏ –≤ –∏–Ω—Ç–µ—Ä–µ—Å–∏—Ç–µ (–¥–æ 256)
    interests_len = len(survey.interests or "") / 256

    # üìä –ë—Ä–æ–π –∏–∑–±—Ä–∞–Ω–∏ —Ä–µ–∫–ª–∞–º–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–Ω (–ø—Ä–∏–µ–º–∞–º–µ, —á–µ —Å–∞ –¥–æ 3)
    ad_count = len((survey.selected_ads or "").split(',')) / 3

    # üíª –ö–æ–¥–∏—Ä–∞–Ω–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ—Ç–æ:
    # PC ‚Üí 0, Mobile ‚Üí 0.5, Tablet ‚Üí 1
    device_score = 0 if survey.device == 'PC' else (1 if survey.device == 'Mobile' else 2) / 2

    # üßÆ –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –≤—Ö–æ–¥–µ–Ω –≤–µ–∫—Ç–æ—Ä –∑–∞ –º–æ–¥–µ–ª–∞
    X = np.array([
        survey.age / 100,                  # –í—ä–∑—Ä–∞—Å—Ç—Ç–∞ –∫–∞—Ç–æ —Å—Ç–æ–π–Ω–æ—Å—Ç –æ—Ç 0 –¥–æ 1
        survey.daily_online_hours / 24,   # –û–Ω–ª–∞–π–Ω –≤—Ä–µ–º–µ –∫–∞—Ç–æ —Å—Ç–æ–π–Ω–æ—Å—Ç –æ—Ç 0 –¥–æ 1
        device_score,                     # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∫–∞—Ç–æ —á–∏—Å–ª–æ–≤–∞ —Å—Ç–æ–π–Ω–æ—Å—Ç
        interests_len,                    # –î—ä–ª–∂–∏–Ω–∞ –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å–∏—Ç–µ (0 –¥–æ 1)
        ad_count                          # –ë—Ä–æ–π —Ä–µ–∫–ª–∞–º–∏ (0 –¥–æ 1)
    ]).reshape(1, -1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–≤–∞–Ω–µ –≤—ä–≤ —Ñ–æ—Ä–º–∞—Ç [1, N]

    # –í—Ä—ä—â–∞–Ω–µ –Ω–∞ –∑–∞–∫—Ä—ä–≥–ª–µ–Ω–∞—Ç–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç (–ø—Ä–∏–º–µ—Ä–Ω–æ: 0.76)
    return round(float(model.predict_proba(X)[0]), 2)
