# üì¶ –ò–º–ø–æ—Ä—Ç –Ω–∞ NumPy –∑–∞ —á–∏—Å–ª–æ–≤–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏ –º–∞—Å–∏–≤–∏
import numpy as np
# üíæ –ò–º–ø–æ—Ä—Ç –Ω–∞ joblib –∑–∞ –∑–∞–ø–∏—Å –∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏ –º–æ–¥–µ–ª–∏
import joblib
# üìÇ –ó–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞–Ω–µ –Ω–∞ —Ñ–∞–π–ª
import os
<<<<<<< HEAD
import json
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, log_loss
from sklearn.model_selection import train_test_split
from app import db
from app.models import SurveyResponse, ModelWeights, AdClick
import pandas as pd

MODEL_PATH = 'instance/model.pkl'

class EnhancedLogisticRegression:
=======
# üìä –ó–∞ –∏–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏
from sklearn.metrics import accuracy_score, mean_squared_error, log_loss
# üìå –ü—ä—Ç –¥–æ —Ñ–∞–π–ª–∞, –≤ –∫–æ–π—Ç–æ —Å–µ –ø–∞–∑–∏ –º–æ–¥–µ–ª—ä—Ç
MODEL_PATH = 'instance/model.pkl'

class SimpleLogisticRegression:
>>>>>>> b9fdf71cb45a9dbe4f7b1a8245ed9de52c9b2818
    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ç–µ–≥–ª–∞—Ç–∞ –∏ bias-–∞ (—Ç–µ —â–µ —Å–µ –æ–±—É—á–∞—Ç –ø—Ä–∏ –Ω—É–∂–¥–∞)
        self.weights = None
        self.bias = None
<<<<<<< HEAD
        self.feature_names = [
            'age_normalized', 'daily_online_hours_normalized', 'device_score',
            'interests_length', 'ad_count', 'streaming_apps_count_normalized',
            'video_clip_length_normalized'
        ]
        self.metrics = {}
=======
        self.training_history = {'loss': [], 'accuracy': []}
>>>>>>> b9fdf71cb45a9dbe4f7b1a8245ed9de52c9b2818

    def sigmoid(self, z):
        # –°–∏–≥–º–æ–∏–¥–∞ ‚Äî –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞ –ª–æ–≥–∏—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å–∏—è
        return 1 / (1 + np.exp(-z))

    def predict_proba(self, X):
        # –ê–∫–æ –º–æ–¥–µ–ª—ä—Ç –Ω–µ –µ –æ–±—É—á–µ–Ω, –æ–±—É—á–∞–≤–∞–º–µ —Å dummy –¥–∞–Ω–Ω–∏
        if self.weights is None or self.bias is None:
<<<<<<< HEAD
            self.load_from_db()
            if self.weights is None:
                # Initialize with dummy data if no trained model exists
                X_dummy = np.random.rand(10, len(self.feature_names))
                y_dummy = (X_dummy[:, 0] + X_dummy[:, 1] > 1).astype(int)
                self.fit(X_dummy, y_dummy)
=======
            X_dummy = np.random.rand(10, X.shape[1])  # 10 –ø—Ä–∏–º–µ—Ä–∞ —Å—ä—Å —Å—ä—â–∏—Ç–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            y_dummy = (X_dummy[:, 0] + X_dummy[:, 1] > 1).astype(int)  # –ø—Ä–æ—Å—Ç–∞ –ª–æ–≥–∏–∫–∞ –∑–∞ target
            self.fit(X_dummy, y_dummy)

        # –í—Ä—ä—â–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ (–º–µ–∂–¥—É 0 –∏ 1)
>>>>>>> b9fdf71cb45a9dbe4f7b1a8245ed9de52c9b2818
        return self.sigmoid(np.dot(X, self.weights) + self.bias)

    def predict(self, X):
        # –í—Ä—ä—â–∞ 0 –∏–ª–∏ 1 —Å–ø–æ—Ä–µ–¥ —Ç–æ–≤–∞ –¥–∞–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ –µ >= 0.5
        return (self.predict_proba(X) >= 0.5).astype(int)

    def fit(self, X, y, lr=0.1, epochs=100):
        # –û–±—É—á–µ–Ω–∏–µ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–µ–Ω —Å–ø—É—Å–∫
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
<<<<<<< HEAD
        for _ in range(epochs):
=======
        # –ò–∑—á–∏—Å—Ç–≤–∞–Ω–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—è—Ç–∞ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ
        self.training_history = {'loss': [], 'accuracy': []}

        for epoch in range(epochs):
>>>>>>> b9fdf71cb45a9dbe4f7b1a8245ed9de52c9b2818
            linear_model = np.dot(X, self.weights) + self.bias
            y_predicted = self.sigmoid(linear_model)

            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–∏ –∑–∞ —Ç–µ–≥–ª–∞—Ç–∞ –∏ bias-–∞
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)

            # –û–±–Ω–æ–≤—è–≤–∞–Ω–µ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—Ç–µ
            self.weights -= lr * dw
            self.bias -= lr * db
            
            # –ò–∑—á–∏—Å–ª—è–≤–∞–Ω–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
            if epoch % 10 == 0:  # –ó–∞–ø–∏—Å–≤–∞–º–µ –Ω–∞ –≤—Å–µ–∫–∏ 10 –µ–ø–æ—Ö–∏
                loss = log_loss(y, y_predicted, labels=[0, 1])
                accuracy = accuracy_score(y, self.predict(X))
                self.training_history['loss'].append(loss)
                self.training_history['accuracy'].append(accuracy)

    def evaluate(self, X_test, y_test):
        """–û—Ü–µ–Ω—è–≤–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞ —Å —Ä–∞–∑–ª–∏—á–Ω–∏ –º–µ—Ç—Ä–∏–∫–∏"""
        y_pred = self.predict(X_test)
        y_pred_proba = self.predict_proba(X_test)
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'mse': mean_squared_error(y_test, y_pred_proba),
            'log_loss': log_loss(y_test, y_pred_proba, labels=[0, 1]),
            'training_loss': self.training_history['loss'][-1] if self.training_history['loss'] else None,
            'training_accuracy': self.training_history['accuracy'][-1] if self.training_history['accuracy'] else None
        }
        
        return metrics

    def calculate_metrics(self, X, y_true):
        y_pred = self.predict(X)
        y_pred_proba = self.predict_proba(X)
        
        self.metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1_score': f1_score(y_true, y_pred, zero_division=0),
            'logloss': log_loss(y_true, y_pred_proba),
            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()
        }
        return self.metrics

    def information_gain(self, X, y, feature_idx):
        """Calculate information gain for a specific feature"""
        from sklearn.tree import DecisionTreeClassifier
        
        # Calculate entropy of the target variable
        def entropy(y):
            classes, counts = np.unique(y, return_counts=True)
            probabilities = counts / len(y)
            return -np.sum(probabilities * np.log2(probabilities + 1e-10))
        
        initial_entropy = entropy(y)
        
        # Split on the feature
        feature_values = X[:, feature_idx]
        unique_values = np.unique(feature_values)
        
        weighted_entropy = 0
        for value in unique_values:
            mask = feature_values == value
            subset_y = y[mask]
            if len(subset_y) > 0:
                weighted_entropy += (len(subset_y) / len(y)) * entropy(subset_y)
        
        return initial_entropy - weighted_entropy

    def calculate_feature_importance(self, X, y):
        """Calculate information gain for all features"""
        importance = {}
        for i, feature_name in enumerate(self.feature_names):
            importance[feature_name] = self.information_gain(X, y, i)
        return importance

    def save_to_db(self):
        """Save model weights and metrics to database"""
        weights_json = json.dumps(self.weights.tolist())
        
        model_weights = ModelWeights(
            weights=weights_json,
            bias=float(self.bias),
            accuracy=self.metrics.get('accuracy', 0.0),
            precision=self.metrics.get('precision', 0.0),
            recall=self.metrics.get('recall', 0.0),
            f1_score=self.metrics.get('f1_score', 0.0),
            logloss=self.metrics.get('logloss', 0.0)
        )
        
        db.session.add(model_weights)
        db.session.commit()

    def load_from_db(self):
        """Load the latest model weights from database"""
        latest_model = ModelWeights.query.order_by(ModelWeights.training_date.desc()).first()
        if latest_model:
            self.weights = np.array(json.loads(latest_model.weights))
            self.bias = latest_model.bias
            self.metrics = {
                'accuracy': latest_model.accuracy,
                'precision': latest_model.precision,
                'recall': latest_model.recall,
                'f1_score': latest_model.f1_score,
                'logloss': latest_model.logloss
            }

    def save(self):
        # –ó–∞–ø–∏—Å–≤–∞–Ω–µ –Ω–∞ —Ç–µ–≥–ª–∞—Ç–∞ –∏ bias-–∞ –≤—ä–≤ —Ñ–∞–π–ª
        joblib.dump((self.weights, self.bias, self.training_history), MODEL_PATH)

    def load(self):
        # –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞, –∞–∫–æ —Ñ–∞–π–ª—ä—Ç —Å—ä—â–µ—Å—Ç–≤—É–≤–∞
        if os.path.exists(MODEL_PATH):
            loaded_data = joblib.load(MODEL_PATH)
            if len(loaded_data) == 3:
                self.weights, self.bias, self.training_history = loaded_data
            else:
                # Backward compatibility
                self.weights, self.bias = loaded_data
                self.training_history = {'loss': [], 'accuracy': []}


def prepare_training_data():
    """Prepare training data from survey responses and ad clicks"""
    surveys = SurveyResponse.query.all()
    
    if len(surveys) < 10:  # Need minimum data for training
        return None, None
    
    X_data = []
    y_data = []
    
    for survey in surveys:
        # Get ad clicks for this user
        ad_clicks = AdClick.query.filter_by(user_id=survey.user_id).count()
        has_clicked = 1 if ad_clicks > 0 else 0
        
        # Prepare features
        interests_len = len(survey.interests or "") / 256
        ad_count = len((survey.selected_ads or "").split(',')) / 3
        device_score = 0 if survey.device == 'PC' else (1 if survey.device == 'Mobile' else 2) / 2
        
        features = [
            survey.age / 100,  # Normalized age
            survey.daily_online_hours / 24,  # Normalized hours
            device_score,
            interests_len,
            ad_count,
            survey.streaming_apps_count / 20,  # Normalized streaming apps
            survey.video_clip_length / 300  # Normalized video length
        ]
        
        X_data.append(features)
        y_data.append(has_clicked)
    
    return np.array(X_data), np.array(y_data)

def train_model():
    """Train the model with current data"""
    X, y = prepare_training_data()
    
    if X is None or len(X) < 10:
        return None
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Train model
    model = EnhancedLogisticRegression()
    model.fit(X_train, y_train, epochs=200)
    
    # Calculate metrics
    metrics = model.calculate_metrics(X_test, y_test)
    
    # Save to database
    model.save_to_db()
    
    return model, metrics

model = EnhancedLogisticRegression()

def predict_click_probability(survey):
<<<<<<< HEAD
    """Predict click probability for a survey response"""
    interests_len = len(survey.interests or "") / 256
    ad_count = len((survey.selected_ads or "").split(',')) / 3
=======
    # üìè –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–Ω–µ: –±—Ä–æ–π —Å–∏–º–≤–æ–ª–∏ –≤ –∏–Ω—Ç–µ—Ä–µ—Å–∏—Ç–µ (–¥–æ 256)
    interests_len = len(survey.interests or "") / 256

    # üìä –ë—Ä–æ–π –∏–∑–±—Ä–∞–Ω–∏ —Ä–µ–∫–ª–∞–º–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–Ω (–ø—Ä–∏–µ–º–∞–º–µ, —á–µ —Å–∞ –¥–æ 3)
    ad_count = len((survey.selected_ads or "").split(',')) / 3

    # üíª –ö–æ–¥–∏—Ä–∞–Ω–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ—Ç–æ:
    # PC ‚Üí 0, Mobile ‚Üí 0.5, Tablet ‚Üí 1
>>>>>>> b9fdf71cb45a9dbe4f7b1a8245ed9de52c9b2818
    device_score = 0 if survey.device == 'PC' else (1 if survey.device == 'Mobile' else 2) / 2

    # üßÆ –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –≤—Ö–æ–¥–µ–Ω –≤–µ–∫—Ç–æ—Ä –∑–∞ –º–æ–¥–µ–ª–∞
    X = np.array([
<<<<<<< HEAD
        survey.age / 100,
        survey.daily_online_hours / 24,
        device_score,
        interests_len,
        ad_count,
        survey.streaming_apps_count / 20,
        survey.video_clip_length / 300
    ]).reshape(1, -1)
=======
        survey.age / 100,                  # –í—ä–∑—Ä–∞—Å—Ç—Ç–∞ –∫–∞—Ç–æ —Å—Ç–æ–π–Ω–æ—Å—Ç –æ—Ç 0 –¥–æ 1
        survey.daily_online_hours / 24,   # –û–Ω–ª–∞–π–Ω –≤—Ä–µ–º–µ –∫–∞—Ç–æ —Å—Ç–æ–π–Ω–æ—Å—Ç –æ—Ç 0 –¥–æ 1
        device_score,                     # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∫–∞—Ç–æ —á–∏—Å–ª–æ–≤–∞ —Å—Ç–æ–π–Ω–æ—Å—Ç
        interests_len,                    # –î—ä–ª–∂–∏–Ω–∞ –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å–∏—Ç–µ (0 –¥–æ 1)
        ad_count                          # –ë—Ä–æ–π —Ä–µ–∫–ª–∞–º–∏ (0 –¥–æ 1)
    ]).reshape(1, -1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–≤–∞–Ω–µ –≤—ä–≤ —Ñ–æ—Ä–º–∞—Ç [1, N]
>>>>>>> b9fdf71cb45a9dbe4f7b1a8245ed9de52c9b2818

    # –í—Ä—ä—â–∞–Ω–µ –Ω–∞ –∑–∞–∫—Ä—ä–≥–ª–µ–Ω–∞—Ç–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç (–ø—Ä–∏–º–µ—Ä–Ω–æ: 0.76)
    return round(float(model.predict_proba(X)[0]), 2)
<<<<<<< HEAD

def get_model_metrics():
    """Get the latest model metrics"""
    latest_model = ModelWeights.query.order_by(ModelWeights.training_date.desc()).first()
    if latest_model:
        return {
            'accuracy': latest_model.accuracy,
            'precision': latest_model.precision,
            'recall': latest_model.recall,
            'f1_score': latest_model.f1_score,
            'logloss': latest_model.logloss,
            'training_date': latest_model.training_date
        }
    return None

def get_feature_importance():
    """Calculate and return feature importance"""
    X, y = prepare_training_data()
    if X is None:
        return {}
    
    model = EnhancedLogisticRegression()
    return model.calculate_feature_importance(X, y)

=======
>>>>>>> b9fdf71cb45a9dbe4f7b1a8245ed9de52c9b2818
